{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import csv\n",
    "import spacy \n",
    "import time\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "filepath = os.getenv('FILEPATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "The corpus is formatted as a CSV and contains the following fields:\n",
    "\n",
    "    id\n",
    "    domain\n",
    "    type\n",
    "    url\n",
    "    content\n",
    "    scraped_at\n",
    "    inserted_at\n",
    "    updated_at\n",
    "    title\n",
    "    authors\n",
    "    keywords\n",
    "    meta_keywords\n",
    "    meta_description\n",
    "    tags\n",
    "    summary\n",
    "    source (opensources, nytimes, or webhose)\n",
    "\"\"\"\n",
    "csv.field_size_limit(100000000)\n",
    "\n",
    "\n",
    "selected_columns = [\"domain\", \"title\", \"authors\", \"type\", \"content\", \"url\"]\n",
    "sample_chunks = []\n",
    "for chunk in pd.read_csv(filepath, usecols=selected_columns, chunksize=100000, on_bad_lines='warn', engine='python'):\n",
    "    sample = chunk.sample(frac=0.1, random_state=42)\n",
    "    sample_chunks.append(sample)\n",
    "    df = pd.concat(sample_chunks, ignore_index=True)\n",
    "\n",
    "train_dataframe, temp_dataframe = train_test_split(df, test_size=0.2, random_state=42) # 80% for training and temp for validation and testing\n",
    "validation_dataframe, test_dataframe = train_test_split(temp_dataframe, test_size=0.5, random_state=42) # splitting the temp data into 10% for validation and 10% for testing\n",
    "\n",
    "train_dataframe.to_csv(\"data/train_data.csv\", chunksize=100000)\n",
    "validation_dataframe.to_csv(\"data/validation_data.csv\", chunksize=100000)\n",
    "test_dataframe.to_csv(\"data/test_data.csv\", chunksize=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"]) # https://www.geeksforgeeks.org/tokenization-using-spacy-library/\n",
    "output_file = \"processed_data.txt\"\n",
    "selected_columns = [\"domain\", \"title\", \"authors\", \"type\", \"content\", \"url\"]\n",
    "\n",
    "# counter info https://www.geeksforgeeks.org/python-counter-objects-elements/\n",
    "lemmatized_vocab_counter = Counter() # number of lemmatized words without stopwords \n",
    "stopword_counter = Counter() \n",
    "filtered_vocab_counter = Counter() # number of words without stopwords\n",
    "original_vocab = set()\n",
    "\n",
    "# I used chatgpt here for understanding how to use spacy and processing the texts in batches\n",
    "# spacy docs: https://spacy.io/usage/processing-pipelines \n",
    "def preprocess(texts):\n",
    "    for doc in nlp.pipe(texts, batch_size=500, n_process=4): # batch size is the number of texts to process at once\n",
    "\n",
    "        for token in doc: # token is a single word/punctuation in a list of tokens\n",
    "            if token.is_alpha: # check if token is a word\n",
    "                original_vocab.add(token.text.lower())\n",
    "                word = token.text.lower()\n",
    "                if (token.is_stop): # if the token is a stopwword, add to counter\n",
    "                    stopword_counter[word] += 1\n",
    "                else: # otherwise update the filtered vocab counter and lemmatized vocab counter\n",
    "                    filtered_vocab_counter[word] += 1\n",
    "                    lemmatized_vocab_counter[token.lemma_.lower()] += 1 \n",
    "        \n",
    "        yield \" \".join(token.lemma_.lower() for token in doc if token.is_alpha and not token.is_stop)\n",
    "\n",
    "input_files = [\"data/train_data.csv\", \"data/validation_data.csv\", \"data/test_data.csv\"]\n",
    "output_files = [\"data/processed_train.csv\", \"data/processed_validation.csv\", \"data/processed_test.csv\"]\n",
    "\n",
    "for input_file, output_file in zip(input_files, output_files):\n",
    "    print(f\"Processing {input_file}\")\n",
    "    first_chunk = True\n",
    "\n",
    "    # writing the processed text data into another file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f_out: \n",
    "        for chunk in tqdm(pd.read_csv(input_file, encoding=\"utf-8\", usecols=selected_columns, on_bad_lines='warn', chunksize=1024, engine='python')):\n",
    "            start = time.time() # timing the chunk processing for debugging\n",
    "\n",
    "            # adding label column which tells whether the article is reliable (=1) or not (=0)\n",
    "            chunk[\"label\"] = chunk[\"type\"].apply(lambda x: 1 if str(x).strip().lower() == \"reliable\" else 0)\n",
    "            # collecting yields into a list\n",
    "            texts = chunk[\"content\"].fillna(\"\").astype(str).tolist() # content column to a list\n",
    "            processed = list(preprocess(texts)) # processing the list\n",
    "            chunk[\"processed_text\"] = processed # assigning back to chunk\n",
    "\n",
    "            print(f\"Chunk processed in {time.time() - start:.2f}s\")\n",
    "            # appending the chunk to the output file\n",
    "            chunk[[\"processed_text\", \"label\", \"type\", \"domain\", \"title\", \"url\", \"authors\"]].to_csv(f_out, mode=\"a\", header=first_chunk, index=False) \n",
    "            first_chunk = False # for the header \n",
    "\n",
    "\n",
    "print(\"Preprocessing done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/processed_train.csv\")  \n",
    "df_valid = pd.read_csv(\"data/processed_validation.csv\")\n",
    "df_test = pd.read_csv(\"data/processed_test.csv\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing reduction rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "original_vocab_size = len(original_vocab) \n",
    "filtered_vocab_size = len(filtered_vocab_counter)\n",
    "lemmatized_vocab_size = len(lemmatized_vocab_counter)\n",
    "\n",
    "# reduction rate after stopword removal\n",
    "stopword_reduction_rate = ((original_vocab_size - filtered_vocab_size) / original_vocab_size) * 100\n",
    "\n",
    "# calculating elmamtized reduction rate\n",
    "lemmatized_reduction_rate = ((filtered_vocab_size - lemmatized_vocab_size) / filtered_vocab_size) * 100\n",
    "\n",
    "print(f\"Original vocabulary size: {original_vocab_size}\")\n",
    "print(f\"Vocabulary size after removing stopwords: {filtered_vocab_size}\")\n",
    "print(f\"Vocabulary size after lemmatization: {lemmatized_vocab_size}\")\n",
    "print(f\"Stopword reduction rate: {stopword_reduction_rate:.2f}%\")\n",
    "print(f\"lemmatized reduction rate: {lemmatized_reduction_rate:.2f}%\")\n",
    "\n",
    "with open(\"data/preprocessing_stats.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"Original vocabulary size: {original_vocab_size}\\n\")\n",
    "    f.write(f\"Vocabulary size after removing stopwords: {filtered_vocab_size}\\n\")\n",
    "    f.write(f\"Vocabulary size after lemmatization: {lemmatized_vocab_size}\\n\")\n",
    "    f.write(f\"Stopword reduction rate: {stopword_reduction_rate:.2f}%\\n\")\n",
    "    f.write(f\"Lemmatized reduction rate: {lemmatized_reduction_rate:.2f}%\\n\")\n",
    "\n",
    "print(df_train.head()) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring train data (task 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "# Precentage distribution of labels in the datasetr\n",
    "type_distribution = df_train[\"type\"].value_counts(normalize=True) * 100 # counting the distribution\n",
    "type_distribution.sort_values().plot(kind=\"barh\", figsize=(10, 6), color=\"darkblue\") # plotting a horizontal bar\n",
    "\n",
    "plt.xlabel(\"Percentage\")\n",
    "plt.ylabel(\"News Type\")\n",
    "plt.title(\"Distribution of News Types in Dataset\")\n",
    "plt.show()\n",
    "\n",
    "# visualizing the distribution of words in reliable vs non-reliable news\n",
    "reliable_texts = df_train[df_train[\"label\"] == 1][\"processed_text\"].dropna().astype(str)\n",
    "other_texts = df_train[df_train[\"label\"] != 1][\"processed_text\"].dropna().astype(str)\n",
    "\n",
    "reliable_counter = Counter(\" \".join(reliable_texts).split())\n",
    "other_counter = Counter(\" \".join(other_texts).split())\n",
    "\n",
    "# counting the top 10 words in reliable and other news\n",
    "reliable_top10 = reliable_counter.most_common(20)\n",
    "other_top10 = other_counter.most_common(20)\n",
    "\n",
    "# unzipping the word and count pairs\n",
    "words_r, counts_r = zip(*reliable_top10) # https://www.w3schools.com/python/ref_func_zip.asp\n",
    "words_o, counts_o = zip(*other_top10)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot reliable\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(words_r[::-1], counts_r[::-1], color=\"green\")\n",
    "plt.title(\"Top 10 Words in Reliable News\")\n",
    "\n",
    "# Plot other\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(words_o[::-1], counts_o[::-1], color=\"red\")\n",
    "plt.title(\"Top 10 Words in Other News\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
